{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5OKQviu1jhf+85rlPE23c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pj0620/Deep-Learning-EMG-Classification/blob/master/archimedes_emg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ygy3xaTtizpy"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "dlqw4JG44JVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Data"
      ],
      "metadata": {
        "id": "e6d-hoVX4gPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "id": "bDtTlPKagiee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b10408-7b3c-439c-b8eb-d2755914db79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0     1     2     3     4     5     6     7     8     9   ...    89  \\\n",
            "0  2910  2992  3019  2927  3026  3003  2941  3035  2928  3046  ...  2984   \n",
            "1  3044  2959  3006  2956  3014  2896  3042  2991  2993  3063  ...  3019   \n",
            "2  2895  2939  2905  3003  2889  2978  3024  2992  2922  2960  ...  3006   \n",
            "3  2926  2928  3003  2998  2943  3022  2933  3079  2927  3031  ...  2992   \n",
            "4  2931  3025  3014  3051  3014  2928  2959  2908  3025  2969  ...  2967   \n",
            "\n",
            "     90    91    92    93    94    95    96    97    98  \n",
            "0  3063  2948  3044  2963  2900  2921  2977  2983  2979  \n",
            "1  2970  3021  2920  2977  2992  3003  2979  3023  2982  \n",
            "2  2958  3005  2956  2989  3031  3014  3011  2983  3027  \n",
            "3  2921  2990  2987  3008  2945  3041  2907  2992  2963  \n",
            "4  2944  2955  2951  2948  2989  2992  2919  2958  2916  \n",
            "\n",
            "[5 rows x 99 columns]\n"
          ]
        }
      ],
      "source": [
        "control_raw_data = pd.read_csv('drive/MyDrive/EMG/control.csv', header=None)\n",
        "clench_raw_data = pd.read_csv('drive/MyDrive/EMG/clench_grabbing_button_short.csv', header=None)\n",
        "\n",
        "# clench2.csv -> 99.14285714285714\n",
        "# clench.csv -> 99.2\n",
        "# clench_grabbing_button.csv -> 100.0%\n",
        "# clench_grabbing_button_short.csv -> 99.733\n",
        "\n",
        "control_raw_data.drop(control_raw_data.columns[len(control_raw_data.columns)-1], axis=1, inplace=True)\n",
        "clench_raw_data.drop(clench_raw_data.columns[len(clench_raw_data.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "print(control_raw_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalize"
      ],
      "metadata": {
        "id": "mmtv35of4lDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(raw_data):\n",
        "  # zeroed = raw_data.sub((raw_data.max(axis=1) - raw_data.min(axis=1))/2 + raw_data.min(axis=1), axis=0)\n",
        "  # scaled = zeroed.div((raw_data.max(axis=1) - raw_data.min(axis=1))/2, axis=0)\n",
        "  # return scaled\n",
        "\n",
        "  zeroed = raw_data.sub((raw_data.max() - raw_data.min())/2 + raw_data.min())\n",
        "  scaled = zeroed.div((raw_data.max() - raw_data.min())/2)\n",
        "  return scaled\n",
        "\n",
        "control_data_preprocessed = normalize(control_raw_data)\n",
        "clench_data_preprocessed = normalize(clench_raw_data)\n",
        "\n",
        "control_data_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "suJFfO9jjK2V",
        "outputId": "45665c1a-5ad4-4c5b-ca9e-91caa0747a36"
      },
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6   \\\n",
              "0   -0.764706  0.170213  0.345455 -0.482759  0.542254  0.033457 -0.044068   \n",
              "1    0.286275 -0.063830  0.227273 -0.232759  0.457746 -0.762082  0.640678   \n",
              "2   -0.882353 -0.205674 -0.690909  0.172414 -0.422535 -0.152416  0.518644   \n",
              "3   -0.639216 -0.283688  0.200000  0.129310 -0.042254  0.174721 -0.098305   \n",
              "4   -0.600000  0.404255  0.300000  0.586207  0.457746 -0.524164  0.077966   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "695  0.184314 -0.184397  0.527273 -0.206897 -0.225352  0.412639 -0.118644   \n",
              "696 -0.200000 -0.141844  0.227273 -0.379310  0.232394  0.115242  0.491525   \n",
              "697 -0.592157  0.092199  0.236364 -0.267241  0.380282 -0.509294 -0.159322   \n",
              "698 -0.113725 -0.063830  0.090909  0.482759 -0.147887  0.301115  0.003390   \n",
              "699 -0.349020  0.375887 -0.772727  0.818966 -0.126761 -0.055762  0.647458   \n",
              "\n",
              "           7         8         9   ...        89        90        91  \\\n",
              "0    0.497630 -0.582609  0.710611  ...  0.095541  0.740741 -0.185455   \n",
              "1    0.080569 -0.017391  0.819936  ...  0.318471 -0.120370  0.345455   \n",
              "2    0.090047 -0.634783  0.157556  ...  0.235669 -0.231481  0.229091   \n",
              "3    0.914692 -0.591304  0.614148  ...  0.146497 -0.574074  0.120000   \n",
              "4   -0.706161  0.260870  0.215434  ... -0.012739 -0.361111 -0.134545   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "695 -0.184834 -0.034783  0.241158  ...  0.343949  0.407407 -0.490909   \n",
              "696 -0.516588 -0.452174  0.961415  ...  0.178344 -0.583333  0.767273   \n",
              "697  0.402844 -0.017391  0.717042  ...  0.248408 -0.407407 -0.192727   \n",
              "698  0.090047  0.304348  0.067524  ...  0.210191 -0.037037 -0.083636   \n",
              "699 -0.213270 -0.095652  0.588424  ...  0.420382 -0.342593  0.243636   \n",
              "\n",
              "           92        93        94        95        96        97        98  \n",
              "0    0.581749 -0.347639 -0.362416 -0.609756  0.043478  0.201389 -0.144186  \n",
              "1   -0.361217 -0.227468  0.255034  0.056911  0.056856  0.479167 -0.116279  \n",
              "2   -0.087452 -0.124464  0.516779  0.146341  0.270903  0.201389  0.302326  \n",
              "3    0.148289  0.038627 -0.060403  0.365854 -0.424749  0.263889 -0.293023  \n",
              "4   -0.125475 -0.476395  0.234899 -0.032520 -0.344482  0.027778 -0.730233  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "695  0.307985 -0.175966  0.221477 -0.317073 -0.210702 -0.076389  0.265116  \n",
              "696 -0.019011  0.038627  0.114094 -0.154472 -0.003344  0.375000  0.181395  \n",
              "697  0.026616  0.158798 -0.087248  0.487805  0.371237 -0.256944  0.376744  \n",
              "698 -0.353612 -0.038627  0.342282 -0.016260  0.344482  0.368056  0.293023  \n",
              "699 -0.300380  0.313305 -0.147651  0.479675  0.076923  0.159722 -0.283721  \n",
              "\n",
              "[700 rows x 99 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40b2ab31-6c12-4261-b0a5-4f0d670c9419\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.764706</td>\n",
              "      <td>0.170213</td>\n",
              "      <td>0.345455</td>\n",
              "      <td>-0.482759</td>\n",
              "      <td>0.542254</td>\n",
              "      <td>0.033457</td>\n",
              "      <td>-0.044068</td>\n",
              "      <td>0.497630</td>\n",
              "      <td>-0.582609</td>\n",
              "      <td>0.710611</td>\n",
              "      <td>...</td>\n",
              "      <td>0.095541</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>-0.185455</td>\n",
              "      <td>0.581749</td>\n",
              "      <td>-0.347639</td>\n",
              "      <td>-0.362416</td>\n",
              "      <td>-0.609756</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.201389</td>\n",
              "      <td>-0.144186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.286275</td>\n",
              "      <td>-0.063830</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>-0.232759</td>\n",
              "      <td>0.457746</td>\n",
              "      <td>-0.762082</td>\n",
              "      <td>0.640678</td>\n",
              "      <td>0.080569</td>\n",
              "      <td>-0.017391</td>\n",
              "      <td>0.819936</td>\n",
              "      <td>...</td>\n",
              "      <td>0.318471</td>\n",
              "      <td>-0.120370</td>\n",
              "      <td>0.345455</td>\n",
              "      <td>-0.361217</td>\n",
              "      <td>-0.227468</td>\n",
              "      <td>0.255034</td>\n",
              "      <td>0.056911</td>\n",
              "      <td>0.056856</td>\n",
              "      <td>0.479167</td>\n",
              "      <td>-0.116279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.882353</td>\n",
              "      <td>-0.205674</td>\n",
              "      <td>-0.690909</td>\n",
              "      <td>0.172414</td>\n",
              "      <td>-0.422535</td>\n",
              "      <td>-0.152416</td>\n",
              "      <td>0.518644</td>\n",
              "      <td>0.090047</td>\n",
              "      <td>-0.634783</td>\n",
              "      <td>0.157556</td>\n",
              "      <td>...</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>-0.231481</td>\n",
              "      <td>0.229091</td>\n",
              "      <td>-0.087452</td>\n",
              "      <td>-0.124464</td>\n",
              "      <td>0.516779</td>\n",
              "      <td>0.146341</td>\n",
              "      <td>0.270903</td>\n",
              "      <td>0.201389</td>\n",
              "      <td>0.302326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.639216</td>\n",
              "      <td>-0.283688</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.129310</td>\n",
              "      <td>-0.042254</td>\n",
              "      <td>0.174721</td>\n",
              "      <td>-0.098305</td>\n",
              "      <td>0.914692</td>\n",
              "      <td>-0.591304</td>\n",
              "      <td>0.614148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146497</td>\n",
              "      <td>-0.574074</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.148289</td>\n",
              "      <td>0.038627</td>\n",
              "      <td>-0.060403</td>\n",
              "      <td>0.365854</td>\n",
              "      <td>-0.424749</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>-0.293023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.404255</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.457746</td>\n",
              "      <td>-0.524164</td>\n",
              "      <td>0.077966</td>\n",
              "      <td>-0.706161</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>0.215434</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012739</td>\n",
              "      <td>-0.361111</td>\n",
              "      <td>-0.134545</td>\n",
              "      <td>-0.125475</td>\n",
              "      <td>-0.476395</td>\n",
              "      <td>0.234899</td>\n",
              "      <td>-0.032520</td>\n",
              "      <td>-0.344482</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>-0.730233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0.184314</td>\n",
              "      <td>-0.184397</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>-0.206897</td>\n",
              "      <td>-0.225352</td>\n",
              "      <td>0.412639</td>\n",
              "      <td>-0.118644</td>\n",
              "      <td>-0.184834</td>\n",
              "      <td>-0.034783</td>\n",
              "      <td>0.241158</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343949</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>-0.490909</td>\n",
              "      <td>0.307985</td>\n",
              "      <td>-0.175966</td>\n",
              "      <td>0.221477</td>\n",
              "      <td>-0.317073</td>\n",
              "      <td>-0.210702</td>\n",
              "      <td>-0.076389</td>\n",
              "      <td>0.265116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>-0.200000</td>\n",
              "      <td>-0.141844</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>-0.379310</td>\n",
              "      <td>0.232394</td>\n",
              "      <td>0.115242</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>-0.516588</td>\n",
              "      <td>-0.452174</td>\n",
              "      <td>0.961415</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178344</td>\n",
              "      <td>-0.583333</td>\n",
              "      <td>0.767273</td>\n",
              "      <td>-0.019011</td>\n",
              "      <td>0.038627</td>\n",
              "      <td>0.114094</td>\n",
              "      <td>-0.154472</td>\n",
              "      <td>-0.003344</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.181395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>-0.592157</td>\n",
              "      <td>0.092199</td>\n",
              "      <td>0.236364</td>\n",
              "      <td>-0.267241</td>\n",
              "      <td>0.380282</td>\n",
              "      <td>-0.509294</td>\n",
              "      <td>-0.159322</td>\n",
              "      <td>0.402844</td>\n",
              "      <td>-0.017391</td>\n",
              "      <td>0.717042</td>\n",
              "      <td>...</td>\n",
              "      <td>0.248408</td>\n",
              "      <td>-0.407407</td>\n",
              "      <td>-0.192727</td>\n",
              "      <td>0.026616</td>\n",
              "      <td>0.158798</td>\n",
              "      <td>-0.087248</td>\n",
              "      <td>0.487805</td>\n",
              "      <td>0.371237</td>\n",
              "      <td>-0.256944</td>\n",
              "      <td>0.376744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>-0.113725</td>\n",
              "      <td>-0.063830</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>-0.147887</td>\n",
              "      <td>0.301115</td>\n",
              "      <td>0.003390</td>\n",
              "      <td>0.090047</td>\n",
              "      <td>0.304348</td>\n",
              "      <td>0.067524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.210191</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.083636</td>\n",
              "      <td>-0.353612</td>\n",
              "      <td>-0.038627</td>\n",
              "      <td>0.342282</td>\n",
              "      <td>-0.016260</td>\n",
              "      <td>0.344482</td>\n",
              "      <td>0.368056</td>\n",
              "      <td>0.293023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>-0.349020</td>\n",
              "      <td>0.375887</td>\n",
              "      <td>-0.772727</td>\n",
              "      <td>0.818966</td>\n",
              "      <td>-0.126761</td>\n",
              "      <td>-0.055762</td>\n",
              "      <td>0.647458</td>\n",
              "      <td>-0.213270</td>\n",
              "      <td>-0.095652</td>\n",
              "      <td>0.588424</td>\n",
              "      <td>...</td>\n",
              "      <td>0.420382</td>\n",
              "      <td>-0.342593</td>\n",
              "      <td>0.243636</td>\n",
              "      <td>-0.300380</td>\n",
              "      <td>0.313305</td>\n",
              "      <td>-0.147651</td>\n",
              "      <td>0.479675</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.159722</td>\n",
              "      <td>-0.283721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows × 99 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40b2ab31-6c12-4261-b0a5-4f0d670c9419')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40b2ab31-6c12-4261-b0a5-4f0d670c9419 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40b2ab31-6c12-4261-b0a5-4f0d670c9419');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 450
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add Classes & Split into test/train/verify data"
      ],
      "metadata": {
        "id": "y7okBJ2X4n-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "control_data_labeled = control_data_preprocessed.copy()\n",
        "clench_data_labeled = clench_data_preprocessed.copy()\n",
        "\n",
        "control_data_labeled['no_clench'] = 1\n",
        "control_data_labeled['clench'] = 0\n",
        "\n",
        "clench_data_labeled['no_clench'] = 0\n",
        "clench_data_labeled['clench'] = 1\n",
        "\n",
        "all_data = clench_data_labeled.append(control_data_labeled, ignore_index=True)\n",
        "all_data = all_data.sample(frac=1, ignore_index=True)\n",
        "all_data\n",
        "\n",
        "# shuffle samples\n",
        "all_data = all_data.sample(frac=1)\n",
        "\n",
        "per_train = 0.6\n",
        "per_test = 0.15\n",
        "N = all_data.shape[0]\n",
        "\n",
        "train_data  = all_data[:int(per_train*N)]\n",
        "test_data   = all_data[int(per_train*N):int((per_train+per_test)*N)]\n",
        "verify_data = all_data[int((per_train+per_test)*N):]\n",
        "\n",
        "train_data.reset_index(inplace=True, drop=True)\n",
        "test_data.reset_index(inplace=True, drop=True)\n",
        "verify_data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "print(f'train: {train_data.shape}, test: {test_data.shape}, verify: {verify_data.shape}')\n",
        "\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "DD7SCbdd4ZyJ",
        "outputId": "9e270c18-9c54-45bd-9019-02d3208309cc"
      },
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (900, 101), test: (225, 101), verify: (375, 101)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    0.043137  0.439716 -0.500000  0.267241 -0.190141  0.434944 -0.098305   \n",
              "1   -0.592157  0.092199  0.236364 -0.267241  0.380282 -0.509294 -0.159322   \n",
              "2   -0.647059  0.773050 -0.209091  0.482759 -0.077465  0.330855  0.193220   \n",
              "3    0.375000  0.825193  0.812977  0.301205 -0.177778  0.388235 -0.105263   \n",
              "4   -0.505882  0.390071 -0.845455  0.689655 -0.119718 -0.263941  0.518644   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "895  0.416667  0.763496  0.828244  0.084337  0.400000 -0.317647  0.500000   \n",
              "896  0.395833  0.794344  0.793893 -0.204819  0.377778 -0.058824  0.131579   \n",
              "897  0.208333  0.742931  0.900763  0.277108  0.044444  0.247059 -0.578947   \n",
              "898  0.011765 -0.134752 -0.190909  0.086207  0.338028 -0.754647  0.647458   \n",
              "899  0.027451 -0.170213 -0.290909  0.344828  0.000000 -0.130112 -0.016949   \n",
              "\n",
              "            7         8         9  ...        91        92        93  \\\n",
              "0   -0.545024 -0.113043  0.549839  ...  0.367273 -0.003802 -0.793991   \n",
              "1    0.402844 -0.017391  0.717042  ... -0.192727  0.026616  0.158798   \n",
              "2   -0.469194  0.417391  0.331190  ...  0.309091  0.087452 -0.072961   \n",
              "3   -0.263158 -0.183099  0.219512  ...  0.270270  0.282051 -0.340659   \n",
              "4   -0.535545 -0.182609  0.363344  ...  0.061818 -0.338403  0.038627   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "895 -0.315789  0.492958 -0.292683  ...  0.000000  0.128205  0.164835   \n",
              "896  0.236842 -0.352113 -0.170732  ... -0.351351  0.358974  0.010989   \n",
              "897 -0.210526  0.323944  0.073171  ... -0.594595  0.000000 -0.010989   \n",
              "898  0.535545 -0.426087  0.665595  ...  0.840000 -0.231939  0.339056   \n",
              "899  0.611374  0.069565  0.286174  ...  0.650909 -0.224335 -0.630901   \n",
              "\n",
              "           94        95        96        97        98  no_clench  clench  \n",
              "0    0.510067 -0.593496  0.117057  0.062500 -0.190698          1       0  \n",
              "1   -0.087248  0.487805  0.371237 -0.256944  0.376744          1       0  \n",
              "2   -0.067114  0.227642  0.444816  0.104167  0.237209          1       0  \n",
              "3   -0.189873  0.645429  0.518519  0.596091  0.394737          0       1  \n",
              "4    0.174497 -0.642276  0.357860 -0.020833  0.265116          1       0  \n",
              "..        ...       ...       ...       ...       ...        ...     ...  \n",
              "895 -0.443038  0.822715 -0.129630  0.680782  0.394737          0       1  \n",
              "896  0.113924  0.883657  0.148148  0.674267  0.421053          0       1  \n",
              "897  0.367089  0.706371  0.296296  0.752443  0.421053          0       1  \n",
              "898 -0.073826  0.008130 -0.304348  0.562500 -0.172093          1       0  \n",
              "899  0.469799 -0.341463  0.270903  0.493056 -0.320930          1       0  \n",
              "\n",
              "[900 rows x 101 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a9b72e1-d7fc-4a88-b461-72123e90ff1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>no_clench</th>\n",
              "      <th>clench</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.439716</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.267241</td>\n",
              "      <td>-0.190141</td>\n",
              "      <td>0.434944</td>\n",
              "      <td>-0.098305</td>\n",
              "      <td>-0.545024</td>\n",
              "      <td>-0.113043</td>\n",
              "      <td>0.549839</td>\n",
              "      <td>...</td>\n",
              "      <td>0.367273</td>\n",
              "      <td>-0.003802</td>\n",
              "      <td>-0.793991</td>\n",
              "      <td>0.510067</td>\n",
              "      <td>-0.593496</td>\n",
              "      <td>0.117057</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>-0.190698</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.592157</td>\n",
              "      <td>0.092199</td>\n",
              "      <td>0.236364</td>\n",
              "      <td>-0.267241</td>\n",
              "      <td>0.380282</td>\n",
              "      <td>-0.509294</td>\n",
              "      <td>-0.159322</td>\n",
              "      <td>0.402844</td>\n",
              "      <td>-0.017391</td>\n",
              "      <td>0.717042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.192727</td>\n",
              "      <td>0.026616</td>\n",
              "      <td>0.158798</td>\n",
              "      <td>-0.087248</td>\n",
              "      <td>0.487805</td>\n",
              "      <td>0.371237</td>\n",
              "      <td>-0.256944</td>\n",
              "      <td>0.376744</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.647059</td>\n",
              "      <td>0.773050</td>\n",
              "      <td>-0.209091</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>-0.077465</td>\n",
              "      <td>0.330855</td>\n",
              "      <td>0.193220</td>\n",
              "      <td>-0.469194</td>\n",
              "      <td>0.417391</td>\n",
              "      <td>0.331190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.309091</td>\n",
              "      <td>0.087452</td>\n",
              "      <td>-0.072961</td>\n",
              "      <td>-0.067114</td>\n",
              "      <td>0.227642</td>\n",
              "      <td>0.444816</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.237209</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.825193</td>\n",
              "      <td>0.812977</td>\n",
              "      <td>0.301205</td>\n",
              "      <td>-0.177778</td>\n",
              "      <td>0.388235</td>\n",
              "      <td>-0.105263</td>\n",
              "      <td>-0.263158</td>\n",
              "      <td>-0.183099</td>\n",
              "      <td>0.219512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.270270</td>\n",
              "      <td>0.282051</td>\n",
              "      <td>-0.340659</td>\n",
              "      <td>-0.189873</td>\n",
              "      <td>0.645429</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.596091</td>\n",
              "      <td>0.394737</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.505882</td>\n",
              "      <td>0.390071</td>\n",
              "      <td>-0.845455</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.263941</td>\n",
              "      <td>0.518644</td>\n",
              "      <td>-0.535545</td>\n",
              "      <td>-0.182609</td>\n",
              "      <td>0.363344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061818</td>\n",
              "      <td>-0.338403</td>\n",
              "      <td>0.038627</td>\n",
              "      <td>0.174497</td>\n",
              "      <td>-0.642276</td>\n",
              "      <td>0.357860</td>\n",
              "      <td>-0.020833</td>\n",
              "      <td>0.265116</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.763496</td>\n",
              "      <td>0.828244</td>\n",
              "      <td>0.084337</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>-0.317647</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.315789</td>\n",
              "      <td>0.492958</td>\n",
              "      <td>-0.292683</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128205</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>-0.443038</td>\n",
              "      <td>0.822715</td>\n",
              "      <td>-0.129630</td>\n",
              "      <td>0.680782</td>\n",
              "      <td>0.394737</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.794344</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>-0.204819</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>-0.058824</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.236842</td>\n",
              "      <td>-0.352113</td>\n",
              "      <td>-0.170732</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.351351</td>\n",
              "      <td>0.358974</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.113924</td>\n",
              "      <td>0.883657</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>0.674267</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.742931</td>\n",
              "      <td>0.900763</td>\n",
              "      <td>0.277108</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.247059</td>\n",
              "      <td>-0.578947</td>\n",
              "      <td>-0.210526</td>\n",
              "      <td>0.323944</td>\n",
              "      <td>0.073171</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.594595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.010989</td>\n",
              "      <td>0.367089</td>\n",
              "      <td>0.706371</td>\n",
              "      <td>0.296296</td>\n",
              "      <td>0.752443</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>0.011765</td>\n",
              "      <td>-0.134752</td>\n",
              "      <td>-0.190909</td>\n",
              "      <td>0.086207</td>\n",
              "      <td>0.338028</td>\n",
              "      <td>-0.754647</td>\n",
              "      <td>0.647458</td>\n",
              "      <td>0.535545</td>\n",
              "      <td>-0.426087</td>\n",
              "      <td>0.665595</td>\n",
              "      <td>...</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>-0.231939</td>\n",
              "      <td>0.339056</td>\n",
              "      <td>-0.073826</td>\n",
              "      <td>0.008130</td>\n",
              "      <td>-0.304348</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>-0.172093</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>0.027451</td>\n",
              "      <td>-0.170213</td>\n",
              "      <td>-0.290909</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.130112</td>\n",
              "      <td>-0.016949</td>\n",
              "      <td>0.611374</td>\n",
              "      <td>0.069565</td>\n",
              "      <td>0.286174</td>\n",
              "      <td>...</td>\n",
              "      <td>0.650909</td>\n",
              "      <td>-0.224335</td>\n",
              "      <td>-0.630901</td>\n",
              "      <td>0.469799</td>\n",
              "      <td>-0.341463</td>\n",
              "      <td>0.270903</td>\n",
              "      <td>0.493056</td>\n",
              "      <td>-0.320930</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a9b72e1-d7fc-4a88-b461-72123e90ff1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a9b72e1-d7fc-4a88-b461-72123e90ff1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a9b72e1-d7fc-4a88-b461-72123e90ff1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction "
      ],
      "metadata": {
        "id": "TDpdKnA2_yZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train AutoEncoder"
      ],
      "metadata": {
        "id": "SFUrzEfBoPaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import progressbar\n",
        "from keras.layers import Input, Dense, LSTM, RepeatVector, LeakyReLU, Dropout\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "latent_dim = 20\n",
        "input_cols = 99\n",
        "\n",
        "# build AutoEncoder\n",
        "inputs = Input(shape=(input_cols,))\n",
        "encoded = Dense(latent_dim)(inputs)\n",
        "decoded = Dense(input_cols, name='reconstructed_layer')(encoded)\n",
        "\n",
        "autoencoder = Model(inputs, decoded)\n",
        "encoder = Model(inputs, encoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "# autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "\n",
        "class_columns = ['no_clench', 'clench']\n",
        "y_train = train_data[class_columns]\n",
        "x_train = train_data.loc[:, ~train_data.columns.isin(class_columns)]\n",
        "y_test = test_data[class_columns]\n",
        "x_test = test_data.loc[:, ~test_data.columns.isin(class_columns)]\n",
        "\n",
        "history = autoencoder.fit( x_train, x_train,\n",
        "                          # epochs=20,\n",
        "                          epochs=200,\n",
        "                          # batch_size=256,\n",
        "                          batch_size=200,\n",
        "                          shuffle=False,\n",
        "                          validation_data=(x_test, x_test),\n",
        "                          verbose=1)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y4wrrk1poWzR",
        "outputId": "e809941f-71f5-4589-f157-4dad9d81a9e5"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 84ms/step - loss: 0.2932 - val_loss: 0.2674\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.2584 - val_loss: 0.2398\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.2336 - val_loss: 0.2195\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.2146 - val_loss: 0.2036\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.1993 - val_loss: 0.1906\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.1865 - val_loss: 0.1794\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1754 - val_loss: 0.1694\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1656 - val_loss: 0.1606\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1569 - val_loss: 0.1529\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.1495 - val_loss: 0.1462\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.1431 - val_loss: 0.1406\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1378 - val_loss: 0.1358\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1334 - val_loss: 0.1317\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.1296 - val_loss: 0.1282\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.1263 - val_loss: 0.1252\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.1235 - val_loss: 0.1225\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1209 - val_loss: 0.1201\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.1186 - val_loss: 0.1180\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1165 - val_loss: 0.1160\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.1146 - val_loss: 0.1142\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1127 - val_loss: 0.1125\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1111 - val_loss: 0.1109\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.1095 - val_loss: 0.1094\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1080 - val_loss: 0.1080\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1066 - val_loss: 0.1067\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1053 - val_loss: 0.1055\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.1041 - val_loss: 0.1043\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.1030 - val_loss: 0.1033\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.1019 - val_loss: 0.1023\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.1009 - val_loss: 0.1013\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1000 - val_loss: 0.1004\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0991 - val_loss: 0.0996\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0982 - val_loss: 0.0988\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0974 - val_loss: 0.0980\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0967 - val_loss: 0.0973\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0960 - val_loss: 0.0967\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0953 - val_loss: 0.0960\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0946 - val_loss: 0.0954\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0940 - val_loss: 0.0948\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0934 - val_loss: 0.0943\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0928 - val_loss: 0.0937\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.0932\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0917 - val_loss: 0.0927\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0911 - val_loss: 0.0923\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0906 - val_loss: 0.0918\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0901 - val_loss: 0.0914\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0897 - val_loss: 0.0909\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0892 - val_loss: 0.0905\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0888 - val_loss: 0.0901\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0883 - val_loss: 0.0898\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0879 - val_loss: 0.0894\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0875 - val_loss: 0.0890\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0871 - val_loss: 0.0887\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0867 - val_loss: 0.0884\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0863 - val_loss: 0.0880\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0860 - val_loss: 0.0877\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0856 - val_loss: 0.0874\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0853 - val_loss: 0.0871\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0849 - val_loss: 0.0868\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0846 - val_loss: 0.0866\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0843 - val_loss: 0.0863\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0840 - val_loss: 0.0860\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0837 - val_loss: 0.0858\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0834 - val_loss: 0.0855\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0831 - val_loss: 0.0853\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0828 - val_loss: 0.0850\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0825 - val_loss: 0.0848\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0822 - val_loss: 0.0846\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0820 - val_loss: 0.0844\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0817 - val_loss: 0.0842\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0815 - val_loss: 0.0839\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0812 - val_loss: 0.0837\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0810 - val_loss: 0.0835\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0808 - val_loss: 0.0834\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0805 - val_loss: 0.0832\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0803 - val_loss: 0.0830\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0801 - val_loss: 0.0828\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0799 - val_loss: 0.0826\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0797 - val_loss: 0.0825\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0795 - val_loss: 0.0823\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0793 - val_loss: 0.0821\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0791 - val_loss: 0.0820\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0789 - val_loss: 0.0818\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0787 - val_loss: 0.0817\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0785 - val_loss: 0.0815\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0783 - val_loss: 0.0814\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0782 - val_loss: 0.0812\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0780 - val_loss: 0.0811\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0778 - val_loss: 0.0809\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0777 - val_loss: 0.0808\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0775 - val_loss: 0.0807\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0773 - val_loss: 0.0805\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0772 - val_loss: 0.0804\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0770 - val_loss: 0.0803\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0769 - val_loss: 0.0802\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0767 - val_loss: 0.0800\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0766 - val_loss: 0.0799\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0764 - val_loss: 0.0798\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0763 - val_loss: 0.0797\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0761 - val_loss: 0.0796\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0760 - val_loss: 0.0795\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0759 - val_loss: 0.0794\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0757 - val_loss: 0.0793\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0756 - val_loss: 0.0791\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0755 - val_loss: 0.0790\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0753 - val_loss: 0.0789\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0752 - val_loss: 0.0788\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0751 - val_loss: 0.0788\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0750 - val_loss: 0.0787\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0749 - val_loss: 0.0786\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0747 - val_loss: 0.0785\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0746 - val_loss: 0.0784\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0745 - val_loss: 0.0783\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0744 - val_loss: 0.0782\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0743 - val_loss: 0.0781\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0742 - val_loss: 0.0780\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0741 - val_loss: 0.0780\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0740 - val_loss: 0.0779\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0739 - val_loss: 0.0778\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0738 - val_loss: 0.0777\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0736 - val_loss: 0.0776\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0735 - val_loss: 0.0776\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0734 - val_loss: 0.0775\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0734 - val_loss: 0.0774\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0733 - val_loss: 0.0773\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0732 - val_loss: 0.0773\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0731 - val_loss: 0.0772\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0730 - val_loss: 0.0771\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0729 - val_loss: 0.0771\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0728 - val_loss: 0.0770\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0727 - val_loss: 0.0769\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0726 - val_loss: 0.0769\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0725 - val_loss: 0.0768\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0724 - val_loss: 0.0767\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0724 - val_loss: 0.0767\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0723 - val_loss: 0.0766\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0722 - val_loss: 0.0766\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0721 - val_loss: 0.0765\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0720 - val_loss: 0.0764\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0720 - val_loss: 0.0764\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0719 - val_loss: 0.0763\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0718 - val_loss: 0.0763\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0717 - val_loss: 0.0762\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0717 - val_loss: 0.0762\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0716 - val_loss: 0.0761\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0715 - val_loss: 0.0761\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0714 - val_loss: 0.0760\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0714 - val_loss: 0.0760\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0713 - val_loss: 0.0759\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0712 - val_loss: 0.0759\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0712 - val_loss: 0.0758\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0711 - val_loss: 0.0758\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0710 - val_loss: 0.0757\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0710 - val_loss: 0.0757\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0709 - val_loss: 0.0757\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0708 - val_loss: 0.0756\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0708 - val_loss: 0.0756\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0707 - val_loss: 0.0755\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0707 - val_loss: 0.0755\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0706 - val_loss: 0.0754\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0705 - val_loss: 0.0754\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0705 - val_loss: 0.0754\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0704 - val_loss: 0.0753\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0704 - val_loss: 0.0753\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0703 - val_loss: 0.0753\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0703 - val_loss: 0.0752\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0702 - val_loss: 0.0752\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0701 - val_loss: 0.0751\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0701 - val_loss: 0.0751\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0700 - val_loss: 0.0751\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0700 - val_loss: 0.0750\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0699 - val_loss: 0.0750\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0699 - val_loss: 0.0750\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0698 - val_loss: 0.0749\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0698 - val_loss: 0.0749\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0697 - val_loss: 0.0749\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0697 - val_loss: 0.0749\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0696 - val_loss: 0.0748\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0696 - val_loss: 0.0748\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0696 - val_loss: 0.0748\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0695 - val_loss: 0.0747\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0695 - val_loss: 0.0747\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0694 - val_loss: 0.0747\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0694 - val_loss: 0.0747\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0693 - val_loss: 0.0746\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0693 - val_loss: 0.0746\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0693 - val_loss: 0.0746\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0692 - val_loss: 0.0746\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0692 - val_loss: 0.0745\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0691 - val_loss: 0.0745\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0691 - val_loss: 0.0745\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0691 - val_loss: 0.0745\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0690 - val_loss: 0.0744\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0690 - val_loss: 0.0744\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0689 - val_loss: 0.0744\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0689 - val_loss: 0.0744\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0689 - val_loss: 0.0743\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0688 - val_loss: 0.0743\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0688 - val_loss: 0.0743\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0688 - val_loss: 0.0743\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.predict(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFCkYX7-8BwM",
        "outputId": "44d7a649-80b1-4de0-f11c-638a164981f6"
      },
      "execution_count": 453,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.17543079,  0.22603881, -1.419278  , ...,  0.46842536,\n",
              "         0.21769093,  0.7686651 ],\n",
              "       [-0.1841636 , -0.46959418,  0.22144747, ...,  0.3540329 ,\n",
              "         0.03468209, -0.33924296],\n",
              "       [-0.51636636,  0.21425967, -0.37769872, ..., -0.20241769,\n",
              "         1.2744    , -0.25263783],\n",
              "       ...,\n",
              "       [-1.558069  ,  1.284247  ,  1.8441864 , ..., -1.4259154 ,\n",
              "        -0.10837084, -0.89384323],\n",
              "       [ 0.46820384,  0.01353657, -0.5627489 , ..., -1.0634803 ,\n",
              "        -0.03937218,  0.7981607 ],\n",
              "       [-0.00408243, -0.5316162 , -0.86387336, ...,  0.92406017,\n",
              "         0.3878819 ,  0.17926303]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 453
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(encoder,to_file='classification_network.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "GOepTHiW6bry",
        "outputId": "fb7ff862-ed63-41a0-c8c9-cfd91b2a14cc"
      },
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 454
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hjorth_complexity(X):\n",
        "    Xp = deriv(X)\n",
        "    if variance(X) == 0:\n",
        "        raise Exception(\"Hjorth Mobility of signal is zero, will result in infinite Hjorth Complexity\")\n",
        "    return hjorth_mobility(Xp)/hjorth_mobility(X)\n",
        "\n",
        "def hjorth_mobility(X):\n",
        "    Xp = deriv(X)\n",
        "    if variance(X) == 0:\n",
        "        raise Exception(\"variance of signal is zero, will result in infinite Hjorth Mobility\")\n",
        "    return (variance(Xp)/variance(X))**0.5\n",
        "\n",
        "def deriv(X):\n",
        "    Xp = np.zeros(shape=(X.shape[0]-1))\n",
        "    for k in range(X.shape[0]-1):\n",
        "        Xp[k] = (X[k+1]-X[k])\n",
        "    return Xp\n",
        "\n",
        "def variance(X):\n",
        "    return np.var(X)\n",
        "\n",
        "def simple_square_integral(X):\n",
        "    return np.square(X).sum()\n",
        "\n",
        "def willison_amplitude(X,threshold=0):\n",
        "    WA = 0\n",
        "    for k in range(X.shape[0]-1):\n",
        "        if abs(X[k+1]-X[k]) > threshold:\n",
        "            WA += 1\n",
        "    return WA/X.shape[0]\n",
        "\n",
        "def root_mean_square(X):\n",
        "    return ((1/X.shape[0])*np.square(X).sum())**0.5\n",
        "\n",
        "def mean(X):\n",
        "    return X.sum()/X.shape[0]\n",
        "\n",
        "def slope_sign_changes(X,threshold=0):\n",
        "    SSC = 0\n",
        "    for k in range(1,len(X)-1):\n",
        "        if (X[k]-X[k-1])*(X[k]-X[k+1]) > threshold:\n",
        "            SSC += 1\n",
        "    return SSC / X.shape[0]\n",
        "\n",
        "def zero_crossing(X,threshold=0):\n",
        "    ZC = 0\n",
        "    for k in range(len(X)-1):\n",
        "        if (X[k] > 0 and X[k+1] < 0) or (X[k] < 0 and X[k+1] > 0 and abs(X[k]-X[k+1])>threshold):\n",
        "            ZC += 1\n",
        "    return ZC\n",
        "\n",
        "def waveform_length(X):\n",
        "    WL = 0\n",
        "    for i in range(len(X)-1):\n",
        "        WL += abs(X[i+1]-X[i])\n",
        "    return WL\n",
        "\n",
        "def mean_absolute_value(X):\n",
        "    return (1/X.shape[0])*np.absolute(X).sum()\n",
        "\n",
        "def extract_predefined_features(raw_data):\n",
        "    output_data = None\n",
        "    feature_funcs = [mean_absolute_value,\n",
        "                     waveform_length,\n",
        "                     zero_crossing,\n",
        "                     slope_sign_changes,\n",
        "                     mean,\n",
        "                     root_mean_square,\n",
        "                     willison_amplitude,\n",
        "                     simple_square_integral,\n",
        "                     variance,\n",
        "                     hjorth_mobility,\n",
        "                     hjorth_complexity]\n",
        "\n",
        "    feature_funcs_cols = ['mean_absolute_value',\n",
        "                     'waveform_length',\n",
        "                     'zero_crossing',\n",
        "                     'slope_sign_changes',\n",
        "                     'mean',\n",
        "                     'root_mean_square',\n",
        "                     'willison_amplitude',\n",
        "                     'simple_square_integral',\n",
        "                     'variance',\n",
        "                     'hjorth_mobility',\n",
        "                     'hjorth_complexity']\n",
        "\n",
        "    new_data = None\n",
        "\n",
        "    for index, row in raw_data.iterrows():\n",
        "      signal = row.to_numpy()\n",
        "      predefined_features =[]\n",
        "      for feature_func in feature_funcs:\n",
        "          predefined_features += [feature_func(signal)]\n",
        "      if new_data is None:\n",
        "        new_data = predefined_features\n",
        "      else:\n",
        "        new_data = np.vstack((new_data,predefined_features))\n",
        "\n",
        "    return pd.DataFrame(new_data, columns = feature_funcs_cols)\n",
        "\n",
        "def extract_features(raw_data):\n",
        "  class_columns = ['no_clench', 'clench']\n",
        "  only_data = raw_data.loc[:, ~raw_data.columns.isin(class_columns)]\n",
        "  only_classes = raw_data[class_columns]\n",
        "  autoencoded_features = encoder.predict(only_data)\n",
        "  autoencoded_df = pd.DataFrame(autoencoded_features)\n",
        "  predef_features = extract_predefined_features(only_data)\n",
        "  return autoencoded_df.join(predef_features).join(only_classes)\n",
        "  # return raw_data\n",
        "\n",
        "train_features = extract_features(train_data)\n",
        "test_features = extract_features(test_data)\n",
        "verify_features = extract_features(verify_data)\n",
        "\n",
        "num_cols = train_features.shape[1] - 2\n",
        "\n",
        "train_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Rdh82kSu_18k",
        "outputId": "3b63da8e-d9fc-48cf-90fe-46293332d24f"
      },
      "execution_count": 455,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    0.175431  0.226039 -1.419278 -0.156984  1.763099 -0.397021 -0.724135   \n",
              "1   -0.184164 -0.469594  0.221447  0.637243  0.684592 -0.881269 -0.343386   \n",
              "2   -0.516366  0.214260 -0.377699  0.010412  0.299455  0.151273  0.005488   \n",
              "3   -0.785166  0.745447  2.178017 -1.096309  1.376893 -1.160824 -0.645634   \n",
              "4    0.100581 -0.201366  0.183967  0.266607  0.317822 -0.940197 -1.176620   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "895 -1.544713  0.090535  2.556072 -0.266557  0.604014 -1.139230 -0.106805   \n",
              "896  0.312650 -0.067376  2.664687 -0.534374  0.533987 -1.899283 -0.683004   \n",
              "897 -1.558069  1.284247  1.844186 -1.286663  0.797222 -0.567725  0.595535   \n",
              "898  0.468204  0.013537 -0.562749 -1.096632 -0.384589 -0.180206  0.491760   \n",
              "899 -0.004082 -0.531616 -0.863873 -0.115181  0.494856 -0.789452 -0.891938   \n",
              "\n",
              "            7         8         9  ...  slope_sign_changes      mean  \\\n",
              "0    0.833560  0.108213 -0.399044  ...            0.737374 -0.038869   \n",
              "1    0.909032 -0.767803 -0.029950  ...            0.727273 -0.017635   \n",
              "2    1.100366 -0.551440 -0.705152  ...            0.707071  0.053073   \n",
              "3   -0.391386  0.205397 -0.206933  ...            0.636364  0.131472   \n",
              "4    1.066087 -0.324634 -0.080255  ...            0.696970 -0.058741   \n",
              "..        ...       ...       ...  ...                 ...       ...   \n",
              "895 -1.479008 -0.410529 -0.275126  ...            0.737374  0.105412   \n",
              "896 -1.657490  0.089671  0.784870  ...            0.616162  0.175216   \n",
              "897 -1.574782  0.078521  0.028493  ...            0.737374  0.131093   \n",
              "898  0.532033 -0.451209 -1.075380  ...            0.727273  0.011414   \n",
              "899  0.612742  0.400583  0.072535  ...            0.747475 -0.010872   \n",
              "\n",
              "     root_mean_square  willison_amplitude  simple_square_integral  variance  \\\n",
              "0            0.402624            0.989899               16.048476  0.160595   \n",
              "1            0.378008            0.989899               14.146148  0.142579   \n",
              "2            0.361423            0.989899               12.932065  0.127810   \n",
              "3            0.451590            0.989899               20.189433  0.186649   \n",
              "4            0.411126            0.979798               16.733457  0.165574   \n",
              "..                ...                 ...                     ...       ...   \n",
              "895          0.459887            0.989899               20.938103  0.200384   \n",
              "896          0.489102            0.989899               23.682866  0.208520   \n",
              "897          0.460055            0.989899               20.953401  0.194465   \n",
              "898          0.391109            0.989899               15.143681  0.152836   \n",
              "899          0.392435            0.989899               15.246532  0.153887   \n",
              "\n",
              "     hjorth_mobility  hjorth_complexity  no_clench  clench  \n",
              "0           1.640578           1.108019          1       0  \n",
              "1           1.540928           1.155567          1       0  \n",
              "2           1.632568           1.129662          1       0  \n",
              "3           1.500669           1.195835          0       1  \n",
              "4           1.634441           1.112700          1       0  \n",
              "..               ...                ...        ...     ...  \n",
              "895         1.529913           1.197243          0       1  \n",
              "896         1.457625           1.209277          0       1  \n",
              "897         1.395958           1.264148          0       1  \n",
              "898         1.582518           1.149871          1       0  \n",
              "899         1.645369           1.107298          1       0  \n",
              "\n",
              "[900 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38b5c57a-4f38-4060-9824-3c9c7485ebb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>slope_sign_changes</th>\n",
              "      <th>mean</th>\n",
              "      <th>root_mean_square</th>\n",
              "      <th>willison_amplitude</th>\n",
              "      <th>simple_square_integral</th>\n",
              "      <th>variance</th>\n",
              "      <th>hjorth_mobility</th>\n",
              "      <th>hjorth_complexity</th>\n",
              "      <th>no_clench</th>\n",
              "      <th>clench</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.175431</td>\n",
              "      <td>0.226039</td>\n",
              "      <td>-1.419278</td>\n",
              "      <td>-0.156984</td>\n",
              "      <td>1.763099</td>\n",
              "      <td>-0.397021</td>\n",
              "      <td>-0.724135</td>\n",
              "      <td>0.833560</td>\n",
              "      <td>0.108213</td>\n",
              "      <td>-0.399044</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737374</td>\n",
              "      <td>-0.038869</td>\n",
              "      <td>0.402624</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>16.048476</td>\n",
              "      <td>0.160595</td>\n",
              "      <td>1.640578</td>\n",
              "      <td>1.108019</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.184164</td>\n",
              "      <td>-0.469594</td>\n",
              "      <td>0.221447</td>\n",
              "      <td>0.637243</td>\n",
              "      <td>0.684592</td>\n",
              "      <td>-0.881269</td>\n",
              "      <td>-0.343386</td>\n",
              "      <td>0.909032</td>\n",
              "      <td>-0.767803</td>\n",
              "      <td>-0.029950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>-0.017635</td>\n",
              "      <td>0.378008</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>14.146148</td>\n",
              "      <td>0.142579</td>\n",
              "      <td>1.540928</td>\n",
              "      <td>1.155567</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.516366</td>\n",
              "      <td>0.214260</td>\n",
              "      <td>-0.377699</td>\n",
              "      <td>0.010412</td>\n",
              "      <td>0.299455</td>\n",
              "      <td>0.151273</td>\n",
              "      <td>0.005488</td>\n",
              "      <td>1.100366</td>\n",
              "      <td>-0.551440</td>\n",
              "      <td>-0.705152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.707071</td>\n",
              "      <td>0.053073</td>\n",
              "      <td>0.361423</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>12.932065</td>\n",
              "      <td>0.127810</td>\n",
              "      <td>1.632568</td>\n",
              "      <td>1.129662</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.785166</td>\n",
              "      <td>0.745447</td>\n",
              "      <td>2.178017</td>\n",
              "      <td>-1.096309</td>\n",
              "      <td>1.376893</td>\n",
              "      <td>-1.160824</td>\n",
              "      <td>-0.645634</td>\n",
              "      <td>-0.391386</td>\n",
              "      <td>0.205397</td>\n",
              "      <td>-0.206933</td>\n",
              "      <td>...</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.131472</td>\n",
              "      <td>0.451590</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>20.189433</td>\n",
              "      <td>0.186649</td>\n",
              "      <td>1.500669</td>\n",
              "      <td>1.195835</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.100581</td>\n",
              "      <td>-0.201366</td>\n",
              "      <td>0.183967</td>\n",
              "      <td>0.266607</td>\n",
              "      <td>0.317822</td>\n",
              "      <td>-0.940197</td>\n",
              "      <td>-1.176620</td>\n",
              "      <td>1.066087</td>\n",
              "      <td>-0.324634</td>\n",
              "      <td>-0.080255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>-0.058741</td>\n",
              "      <td>0.411126</td>\n",
              "      <td>0.979798</td>\n",
              "      <td>16.733457</td>\n",
              "      <td>0.165574</td>\n",
              "      <td>1.634441</td>\n",
              "      <td>1.112700</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>-1.544713</td>\n",
              "      <td>0.090535</td>\n",
              "      <td>2.556072</td>\n",
              "      <td>-0.266557</td>\n",
              "      <td>0.604014</td>\n",
              "      <td>-1.139230</td>\n",
              "      <td>-0.106805</td>\n",
              "      <td>-1.479008</td>\n",
              "      <td>-0.410529</td>\n",
              "      <td>-0.275126</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737374</td>\n",
              "      <td>0.105412</td>\n",
              "      <td>0.459887</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>20.938103</td>\n",
              "      <td>0.200384</td>\n",
              "      <td>1.529913</td>\n",
              "      <td>1.197243</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0.312650</td>\n",
              "      <td>-0.067376</td>\n",
              "      <td>2.664687</td>\n",
              "      <td>-0.534374</td>\n",
              "      <td>0.533987</td>\n",
              "      <td>-1.899283</td>\n",
              "      <td>-0.683004</td>\n",
              "      <td>-1.657490</td>\n",
              "      <td>0.089671</td>\n",
              "      <td>0.784870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616162</td>\n",
              "      <td>0.175216</td>\n",
              "      <td>0.489102</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>23.682866</td>\n",
              "      <td>0.208520</td>\n",
              "      <td>1.457625</td>\n",
              "      <td>1.209277</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>-1.558069</td>\n",
              "      <td>1.284247</td>\n",
              "      <td>1.844186</td>\n",
              "      <td>-1.286663</td>\n",
              "      <td>0.797222</td>\n",
              "      <td>-0.567725</td>\n",
              "      <td>0.595535</td>\n",
              "      <td>-1.574782</td>\n",
              "      <td>0.078521</td>\n",
              "      <td>0.028493</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737374</td>\n",
              "      <td>0.131093</td>\n",
              "      <td>0.460055</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>20.953401</td>\n",
              "      <td>0.194465</td>\n",
              "      <td>1.395958</td>\n",
              "      <td>1.264148</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>0.468204</td>\n",
              "      <td>0.013537</td>\n",
              "      <td>-0.562749</td>\n",
              "      <td>-1.096632</td>\n",
              "      <td>-0.384589</td>\n",
              "      <td>-0.180206</td>\n",
              "      <td>0.491760</td>\n",
              "      <td>0.532033</td>\n",
              "      <td>-0.451209</td>\n",
              "      <td>-1.075380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.011414</td>\n",
              "      <td>0.391109</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>15.143681</td>\n",
              "      <td>0.152836</td>\n",
              "      <td>1.582518</td>\n",
              "      <td>1.149871</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>-0.004082</td>\n",
              "      <td>-0.531616</td>\n",
              "      <td>-0.863873</td>\n",
              "      <td>-0.115181</td>\n",
              "      <td>0.494856</td>\n",
              "      <td>-0.789452</td>\n",
              "      <td>-0.891938</td>\n",
              "      <td>0.612742</td>\n",
              "      <td>0.400583</td>\n",
              "      <td>0.072535</td>\n",
              "      <td>...</td>\n",
              "      <td>0.747475</td>\n",
              "      <td>-0.010872</td>\n",
              "      <td>0.392435</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>15.246532</td>\n",
              "      <td>0.153887</td>\n",
              "      <td>1.645369</td>\n",
              "      <td>1.107298</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b5c57a-4f38-4060-9824-3c9c7485ebb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38b5c57a-4f38-4060-9824-3c9c7485ebb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38b5c57a-4f38-4060-9824-3c9c7485ebb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 455
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "hvP-0v0AUI0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "import random\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import * \n",
        "\n",
        "\n",
        "input_nn = Input(shape=(num_cols,))\n",
        "hidden = Dense(num_cols, activation='relu',name='hidden1')(input_nn)\n",
        "hidden = Dropout(0.1)(hidden)\n",
        "# hidden = Dropout(0.04)(hidden)\n",
        "# hidden = Dense(125, activation='relu')(hidden)\n",
        "# hidden = Dropout(0.07)(hidden)\n",
        "# hidden = Dense(130, activation='relu')(hidden)\n",
        "hidden = Dense(num_cols, activation='relu',name='hidden3')(hidden)\n",
        "output_nn = Dense(2, activation='sigmoid', name='predicted_class')(hidden)\n",
        "\n",
        "neural_network = Model(input_nn, output_nn)\n",
        "neural_network.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "class_columns = ['no_clench', 'clench']\n",
        "y_train = train_features[class_columns]\n",
        "x_train = train_features.loc[:, ~train_features.columns.isin(class_columns)]\n",
        "y_test = test_features[class_columns]\n",
        "x_test = test_features.loc[:, ~test_features.columns.isin(class_columns)]\n",
        "\n",
        "\n",
        "history = neural_network.fit(x_train, y_train,\n",
        "                             epochs=50,\n",
        "                             # batch_size=256,\n",
        "                             batch_size=250,\n",
        "                             shuffle=True,\n",
        "                             validation_data=(x_test, y_test),\n",
        "                             verbose=1)\n",
        "\n",
        "print(\"history = \" + str(history.history))\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z1qiEVzfUM9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acb176ab-7a40-4da8-a944-f09803e244f9"
      },
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 85ms/step - loss: 2.1612 - accuracy: 0.4678 - val_loss: 1.8375 - val_accuracy: 0.4711\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 1.7305 - accuracy: 0.4700 - val_loss: 1.4044 - val_accuracy: 0.4711\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.3166 - accuracy: 0.5011 - val_loss: 1.0181 - val_accuracy: 0.4711\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 1.0784 - accuracy: 0.4833 - val_loss: 0.7465 - val_accuracy: 0.4711\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.8708 - accuracy: 0.4922 - val_loss: 0.6466 - val_accuracy: 0.8178\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.7930 - accuracy: 0.7322 - val_loss: 0.6195 - val_accuracy: 0.5822\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.8018 - accuracy: 0.5644 - val_loss: 0.5864 - val_accuracy: 0.5289\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.7283 - accuracy: 0.6000 - val_loss: 0.5360 - val_accuracy: 0.5956\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.6831 - accuracy: 0.6700 - val_loss: 0.4843 - val_accuracy: 0.8756\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6376 - accuracy: 0.7667 - val_loss: 0.4397 - val_accuracy: 0.9689\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5660 - accuracy: 0.8411 - val_loss: 0.4144 - val_accuracy: 0.9689\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5472 - accuracy: 0.8400 - val_loss: 0.3912 - val_accuracy: 0.9689\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.5199 - accuracy: 0.8589 - val_loss: 0.3656 - val_accuracy: 0.9644\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.4788 - accuracy: 0.8756 - val_loss: 0.3358 - val_accuracy: 0.9733\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.4226 - accuracy: 0.9022 - val_loss: 0.3075 - val_accuracy: 0.9689\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3889 - accuracy: 0.9156 - val_loss: 0.2816 - val_accuracy: 0.9733\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3592 - accuracy: 0.9111 - val_loss: 0.2566 - val_accuracy: 0.9822\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3204 - accuracy: 0.9256 - val_loss: 0.2331 - val_accuracy: 0.9911\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2971 - accuracy: 0.9511 - val_loss: 0.2112 - val_accuracy: 0.9956\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2792 - accuracy: 0.9356 - val_loss: 0.1911 - val_accuracy: 0.9956\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2577 - accuracy: 0.9511 - val_loss: 0.1737 - val_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2178 - accuracy: 0.9644 - val_loss: 0.1582 - val_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2083 - accuracy: 0.9689 - val_loss: 0.1437 - val_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.1963 - accuracy: 0.9622 - val_loss: 0.1293 - val_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.1779 - accuracy: 0.9711 - val_loss: 0.1161 - val_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.1513 - accuracy: 0.9800 - val_loss: 0.1056 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1466 - accuracy: 0.9822 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.1313 - accuracy: 0.9811 - val_loss: 0.0836 - val_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1160 - accuracy: 0.9833 - val_loss: 0.0756 - val_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1113 - accuracy: 0.9878 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1075 - accuracy: 0.9800 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0948 - accuracy: 0.9856 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0835 - accuracy: 0.9889 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0846 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0766 - accuracy: 0.9867 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0747 - accuracy: 0.9911 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0630 - accuracy: 0.9922 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0669 - accuracy: 0.9856 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0527 - accuracy: 0.9956 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0508 - accuracy: 0.9944 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0533 - accuracy: 0.9911 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0468 - accuracy: 0.9933 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0482 - accuracy: 0.9900 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0457 - accuracy: 0.9911 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0413 - accuracy: 0.9956 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0393 - accuracy: 0.9922 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0395 - accuracy: 0.9922 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0319 - accuracy: 0.9956 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0353 - accuracy: 0.9933 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0314 - accuracy: 0.9956 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "history = {'loss': [2.161240339279175, 1.7305370569229126, 1.3165570497512817, 1.0783934593200684, 0.8707800507545471, 0.7930464148521423, 0.8017613291740417, 0.7282993793487549, 0.6831479072570801, 0.6375839710235596, 0.5660366415977478, 0.5471640229225159, 0.5198807716369629, 0.47877445816993713, 0.42257213592529297, 0.38892486691474915, 0.3591631054878235, 0.3204290568828583, 0.2970520257949829, 0.2791516184806824, 0.257686585187912, 0.21775341033935547, 0.20827685296535492, 0.19628170132637024, 0.1779480129480362, 0.15130095183849335, 0.14655108749866486, 0.13127170503139496, 0.11599060893058777, 0.11129152029752731, 0.10745847225189209, 0.09475746005773544, 0.08347678184509277, 0.0846007913351059, 0.0766291618347168, 0.07468864321708679, 0.06304176151752472, 0.06685595214366913, 0.052711181342601776, 0.05080868676304817, 0.053268805146217346, 0.04677518084645271, 0.04823470115661621, 0.04571554809808731, 0.04129498079419136, 0.03927452489733696, 0.039513178169727325, 0.031936150044202805, 0.035284437239170074, 0.03135165572166443], 'accuracy': [0.4677777886390686, 0.4699999988079071, 0.5011110901832581, 0.4833333194255829, 0.49222221970558167, 0.7322221994400024, 0.5644444227218628, 0.6000000238418579, 0.6700000166893005, 0.7666666507720947, 0.8411111235618591, 0.8399999737739563, 0.8588888645172119, 0.8755555748939514, 0.902222216129303, 0.9155555367469788, 0.9111111164093018, 0.9255555272102356, 0.9511111378669739, 0.9355555772781372, 0.9511111378669739, 0.9644444584846497, 0.9688888788223267, 0.9622222185134888, 0.9711111187934875, 0.9800000190734863, 0.9822221994400024, 0.9811111092567444, 0.9833333492279053, 0.9877777695655823, 0.9800000190734863, 0.9855555295944214, 0.9888888597488403, 0.9833333492279053, 0.9866666793823242, 0.9911110997200012, 0.992222249507904, 0.9855555295944214, 0.995555579662323, 0.9944444298744202, 0.9911110997200012, 0.9933333396911621, 0.9900000095367432, 0.9911110997200012, 0.995555579662323, 0.992222249507904, 0.992222249507904, 0.995555579662323, 0.9933333396911621, 0.995555579662323], 'val_loss': [1.8375413417816162, 1.404417872428894, 1.018070936203003, 0.7464756965637207, 0.6465887427330017, 0.6195273399353027, 0.5864430069923401, 0.5359855890274048, 0.4842868149280548, 0.43966570496559143, 0.41438865661621094, 0.39124611020088196, 0.36560872197151184, 0.3357502818107605, 0.3075021505355835, 0.2816142737865448, 0.2566200792789459, 0.23313944041728973, 0.21123766899108887, 0.19112958014011383, 0.1736612170934677, 0.15823672711849213, 0.1436985433101654, 0.12930741906166077, 0.11607316136360168, 0.10560029000043869, 0.09438192844390869, 0.0835634395480156, 0.07555924355983734, 0.0685437023639679, 0.06343824416399002, 0.058987900614738464, 0.052990205585956573, 0.04857286065816879, 0.04514002799987793, 0.04163114354014397, 0.03771049156785011, 0.03480164334177971, 0.03249836713075638, 0.030467860400676727, 0.028559988364577293, 0.026234112679958344, 0.02411695197224617, 0.022246304899454117, 0.020556794479489326, 0.01924171671271324, 0.018826710060238838, 0.017673324793577194, 0.016491739079356194, 0.015339413657784462], 'val_accuracy': [0.47111111879348755, 0.47111111879348755, 0.47111111879348755, 0.47111111879348755, 0.8177777528762817, 0.5822222232818604, 0.5288888812065125, 0.5955555438995361, 0.8755555748939514, 0.9688888788223267, 0.9688888788223267, 0.9688888788223267, 0.9644444584846497, 0.9733333587646484, 0.9688888788223267, 0.9733333587646484, 0.9822221994400024, 0.9911110997200012, 0.995555579662323, 0.995555579662323, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(neural_network,to_file='classification_network.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "A0cWQ2d4VaiF",
        "outputId": "fcb76283-9fda-4458-a9b9-3b91d319f025"
      },
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 457
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "ZDyka5m6VeFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Model"
      ],
      "metadata": {
        "id": "id9qOaV5XGFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_columns = ['no_clench', 'clench']\n",
        "y_verify = verify_features[class_columns].to_numpy()\n",
        "x_verify = verify_features.loc[:, ~verify_features.columns.isin(class_columns)]\n",
        "\n",
        "print(f'verify shape: {verify_features.shape}')\n",
        "\n",
        "predicts = neural_network.predict(x_verify)\n",
        "prediction_col = np.zeros(shape=(predicts.shape[0],1))\n",
        "num_correct = 0\n",
        "confusion_matrix = np.zeros(shape=(2,2))\n",
        "\n",
        "for i in range(predicts.shape[0]):\n",
        "  # print(predicts[i])\n",
        "  is_clutched_predict = 0\n",
        "  if predicts[i][0] < predicts[i][1]:\n",
        "    is_clutched_predict = 1\n",
        "  # print(f'is_clutched_predict: {is_clutched_predict}')\n",
        "  \n",
        "  # is_clutched_actual = 0\n",
        "  # if predicts[i][0] < predicts[i][1]:\n",
        "\n",
        "  # print(y_verify[i])\n",
        "  is_clutched_real = 0\n",
        "  if y_verify[i][0] < y_verify[i][1]:\n",
        "    is_clutched_real = 1\n",
        "  # print(f'is_clutched_real: {is_clutched_real}')\n",
        "\n",
        "  confusion_matrix[is_clutched_real,is_clutched_predict] += 1\n",
        "\n",
        "  if is_clutched_predict == is_clutched_real:\n",
        "    num_correct += 1\n",
        "  \n",
        "print(confusion_matrix)\n",
        "print(f\"testing accuracy = {100*(num_correct/predicts.shape[0])}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKvRNUWMXFL1",
        "outputId": "400860be-903f-413c-b72d-abf5e0288da7"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "verify shape: (375, 33)\n",
            "12/12 [==============================] - 0s 3ms/step\n",
            "[[177.   1.]\n",
            " [  0. 197.]]\n",
            "testing accuracy = 99.73333333333333%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving/Loading model to disk"
      ],
      "metadata": {
        "id": "TSbdbh_xNyM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# serialize model to JSON\n",
        "model_json = neural_network.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "neural_network.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffSEPx2pNBqb",
        "outputId": "c1285c75-5d38-4504-de32-909012b25f95"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OYUQizN2Rc",
        "outputId": "20d44ce6-1c56-4c51-a024-d8b0a9a9071a"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    }
  ]
}